# Complete Configuration Example - Google Gemini

# Service configuration
service_name: general-purpose-chatbot
version: 1.0.0
environment: production

# Server configuration
port: 8080
request_timeout: 30s
idle_timeout: 60s

# LLM Provider selection
llm:
  provider: gemini  # claude, gemini, or openai

# Gemini configuration
gemini:
  api_key: ${GEMINI_API_KEY}
  model: gemini-2.5-flash
  # Optional: for Vertex AI
  project: ${GOOGLE_CLOUD_PROJECT}
  region: ${GOOGLE_CLOUD_REGION}

# Slack configuration
slack:
  bot_token: ${SLACK_BOT_TOKEN}
  app_token: ${SLACK_APP_TOKEN}
  debug: false

# Telegram configuration
telegram:
  bot_token: ${TELEGRAM_BOT_TOKEN}
  debug: false

# Session storage
session:
  backend: local  # local, s3, or memory
  local_dir: ./sessions

# MCP (Model Context Protocol) configuration
mcp:
  enabled: true
  timeout: 30s
  servers:
    filesystem:
      name: filesystem
      enabled: true
      transport: stdio
      command: npx
      args:
        - "-y"
        - "@modelcontextprotocol/server-filesystem"
        - "/data"

# Logging configuration
logging:
  level: info  # debug, info, warn, error
  format: json  # json or text

# Monitoring configuration
monitoring:
  health_check_timeout: 10s
  metrics_enabled: true
  metrics_port: 9090

# Security configuration
security:
  cors_allowed_origins:
    - http://localhost:3000
    - http://localhost:8080
  max_request_size: 10485760  # 10MB
  rate_limit_enabled: true
  rate_limit_rps: 100